= Hawkular OpenShift Agent image:https://travis-ci.org/hawkular/hawkular-openshift-agent.svg["Build Status", link="https://travis-ci.org/hawkular/hawkular-openshift-agent"]

Hawkular OpenShift Agent is a Hawkular feed implemented in the Go Programming Language. Its main purpose is to monitor a node within an OpenShift environment, collecting metrics from Prometheus and/or Jolokia endpoints deployed in one or more pods within the node. It can also be used to collect metrics from endpoints outside of OpenShift. The agent can be deployed inside the OpenShift node it is monitoring, or outside of OpenShift completely.

Note that the agent does not collect or store inventory at this time - this is strictly a metric collection and storage agent that integrates with Hawkular Metrics.

== Building

[NOTE]
These build instructions assume you have the following installed on your system: (1) Go Programming Language, (2) git, and (3) make. To run Hawkular OpenShift Agent after you build it, it is assumed you have OpenShift installed on your system as well. You can find a set of link:http://management-platform.blogspot.com/2016/10/installing-open-shift-origin-and-go-for.html[instructions on how to install Go and OpenShift here].

To build Hawkular OpenShift Agent:

* Clone this repository inside a GOPATH. These instructions will use the example GOPATH of "/source/go/hawkular-openshift-agent"

[source,shell]
----
mkdir -p /source/go
cd /source/go
git clone git@github.com:hawkular/hawkular-openshift-agent.git
export GOPATH=`pwd`/hawkular-openshift-agent
export PATH=${PATH}:${GOPATH}/bin
----

* Install Glide - a Go dependency management tool that Hawkular OpenShift Agent uses to build itself

[source,shell]
----
cd $GOPATH
make install_glide
----

* Tell Glide to install the Hawkular OpenShift Agent dependencies

[source,shell]
----
cd $GOPATH
make install_deps
----

* Build Hawkular OpenShift Agent

[source,shell]
----
cd $GOPATH
make build
----

* At this point you can run the Hawkular OpenShift Agent tests

[source,shell]
----
cd $GOPATH
make test
----

To run Hawkular OpenShift Agent:

[NOTE]
To customize Hawkular OpenShift Agent's configuration so it can point to your OpenShift environment you need to make the appropriate edits to config.yaml. The OpenShift CA cert file can be found in your OpenShift installation at `/var/lib/origin/openshift.local.config/master/ca.crt`. If you installed OpenShift in a VM via vagrant, you can use `vagrant ssh` to get there. If you wish to configure the agent with environment variables as opposed to the config file, see link:https://github.com/hawkular/hawkular-openshift-agent/blob/master/src/github.com/hawkular/hawkular-openshift-agent/config/config.go[here] for the environment variables that the agent looks for.

[source,shell]
----
cd $GOPATH
make run
----

To install the Hawkular OpenShift Agent executable so you can run it outside of the Makefile, run the default target (which is "build") and use the "hawkular-openshift-agent" executable that is placed in the GOPATH /bin directory.

[source,shell]
----
cd $GOPATH
make
${GOPATH}/bin/hawkular-openshift-agent -config <your-config-file>
----

If you don't want to store your token in the YAML file, you can pass it via an environment variable:

[source,shell]
----
K8S_TOKEN=`oc whoami -t` ${GOPATH}/bin/hawkular-openshift-agent -config config.yaml
----

== Configuring OpenShift

When Hawkular OpenShift Agent is monitoring resources running on an OpenShift node, it looks at custom annotations and config maps found in OpenShift to know what to monitor. In effect, the pods tell Hawkular OpenShift Agent what to monitor, and Hawkular OpenShift Agent does it. (Note that where "OpenShift" is mentioned, it is normally synonymous with "Kubernetes" because Hawkular OpenShift Agent is really interfacing with the underlying Kubernetes software that is running in OpenShift)

One caveat must be mentioned up front. Hawkular OpenShift Agent will only monitor a single OpenShift node. If you want to monitor multiple OpenShift nodes, you must run one Hawkular OpenShift Agent process per node.

There are two features in OpenShift that Hawkular OpenShift Agent takes advantage of when it comes to configuring what Hawkular OpenShift Agent should be monitoring - one is pod annotations and the second is project config maps.

=== Pod Annotations

Each pod running on the node has a set of annotations. An annotation is simply a name/value pair. Hawkular OpenShift Agent expects to see an annotation named "hawkular-openshift-agent" on a pod that is to be monitored. If this annotation is missing, it is assumed you do not want Hawkular OpenShift Agent to monitor that pod. The value of this annotation named "hawkular-openshift-agent" is the name of a config map within the pod's project. If the config map is not found in the pod's project, again Hawkular OpenShift Agent will not monitor the pod.

=== Project Config Map

Pods are grouped in what are called "projects" in OpenShift (Kubernetes calls these "namespaces" - if you see "namespace" in the Hawkular OpenShift Agent configuration settings and log messages, realize it is talking about an OpenShift project). Each project has what is called a "config map". Similiar to annotations, config maps contain name/value pairs. The values can be as simple as short strings or as complex as complete YAML or JSON blobs. Because config maps are on projects, they are associated with multiple pods (the pods within the project).

Hawkular OpenShift Agent takes advantage of a project's config maps by using them as places to put YAML configuration for each monitored pod that belongs to the project. Each pod configuration is found in one config map. The config map that Hawkular OpenShift Agent will look for must be named the same as the value found in a pod's "hawkular-openshift-agent" annotation.

=== Config Map Entry Schema

Each Hawkular OpenShift Agent config map must have one and only one entry which must be named "hawkular-openshift-agent". A config map entry is a YAML configuration. The Go representation of the YAML schema is found link:https://github.com/hawkular/hawkular-openshift-agent/blob/master/src/github.com/hawkular/hawkular-openshift-agent/k8s/configmap_entry.go[here].

So, in short, each OpenShift project (aka Kubernetes namespace) will have multiple config maps each with an entry named "hawkular-openshift-agent" where those entries contain YAML configuration containing information about what should be monitored on a pod. A named config map is referenced by a pod's annotation also called "hawkular-openshift-agent".

Hawkular OpenShift Agent examines each pod on the node and by cross-referencing the pod annotations with the project config maps, Hawkular OpenShift Agent knows what it should manage.

=== Example

Suppose you have a node running a project called "my-project" that consists of 3 pods (named "web-pod", "app-pod", and "db-pod"). Suppose you do not want Hawkular OpenShift Agent to monitor the "db-pod" but you do want it to monitor the other two pods in your project.

First create two config maps on your "my-project" that each contain a config map entry that indicate what you want to monitor on your two pods. One way you can do this is create a YAML file that represents your config maps and via the "oc" OpenShift command line tool create the config maps. A sample YAML configuration for the web-pod config map could look like this (the schema of this YAML will change in the future, this is just an example).

[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: my-web-pod-config
  namespace: my-project
data:
  hawkular-openshift-agent: |
    collection_interval_secs: 60
    endpoints:
    - type: prometheus
      protocol: "http"
      port: 8080
      path: /metrics
----

Notice the name given to this config map - "my-web-pod-config". This is the name of the config map, and it is this name that should appear as a value to the "hawkular-openshift-agent" annotation found on the "web-pod" pod. It identifies this config map to Hawkular OpenShift Agent as the one that should be used by that pod. Notice also that the name of the config map entry is fixed and must always be "hawkular-openshift-agent". Next, notice the config map entry here. This defines what are to be monitored. Here you see there is a single endpoint for this pod that will expose Prometheus metrics over http and port 8080 at /metrics. The IP address used will be that of the pod itself and thus need not be specified.

To create this config map, save that YAML to a file and use "oc":

[source,shell]
----
oc create -f my-web-pod-config-map.yaml
----

If you have already created a "my-web-pod-config" config map on your project, you can update it via the "oc replace" command:

[source,shell]
----
oc replace -f my-web-pod-config-map.yaml
----

Now that the config map has been created on your project, you can now add the annotation to the pods that you want to be monitored with the information in that config map. Let's tell Hawkular OpenShift Agent to monitor pod "web-pod" using the configuration named "my-web-pod-config" found in the config map we just created above. We could do something similar for the app-pod (that is, create a config map named, say, "my-app-pod-config" and annotate the app-pod to point to that config map). This can be done with the "oc" command as well.

[source,shell]
----
oc annotate --overwrite pods web-pod hawkular-openshift-agent=my-web-pod-config
oc annotate --overwrite pods app-pod hawkular-openshift-agent=my-app-pod-config
----

Because we do not want to monitor the db-pod, we do not create that annotation on it. This tells Hawkular OpenShift Agent to ignore that pod.

If you want Hawkular OpenShift Agent to stop monitoring a pod, it is as simple as removing the pod's "hawkular-openshift-agent" annotation:

[source,shell]
----
oc annotate pods app-pod hawkular-openshift-agent-
----

== Configuring External Endpoints To Monitor

Hawkular OpenShift Agent is being developed primarily for running within an OpenShift environment. However, strictly speaking, it does not need to run in or monitor OpenShift. You can run Hawkular OpenShift Agent within your own VM, container, or bare metal and configure it to collect metrics from external endpoints you define in the main config.yaml configuration file.

As an example, suppose you want Hawkular OpenShift Agent to scrape metrics from your Prometheus endpoint running at "http://yourcorp.com:9090/metrics" and store those metrics in Hawkular Metrics. You can add an `endpoints` section to your Hawkular OpenShift Agent's configuration file pointing to that endpoint which enables Hawkular OpenShift Agent to begin monitoring that endpoint as soon as Hawkular OpenShift Agent starts. The `endpoints` section of your YAML configuration file could look like this:

[source,yaml]
----
- type: "prometheus"
  url: "http://yourcorp.com:9090/metrics"
  collection_interval_secs: 300
----

== Prometheus Endpoints

A full Prometheus endpoint configuration can look like this:

[source,yaml]
----
- type: "prometheus"
  # If this is an endpoint within an OpenShift pod:
  protocol: https
  port: 9090
  path: /metrics
  # If this is an endpoint running outside of OpenShift:
  #url: "https://yourcorp.com:9090/metrics"
  credentials:
    token: your-bearer-token-here
    #username: your-user
    #password: your-pass
  collection_interval_secs: 300
  metrics:
  - name: go_memstats_last_gc_time_seconds
    id: gc_time_secs
    type: gauge
  - name: go_memstats_frees_total
    type: counter
----

Some things to note about configuring your Prometheus endpoints:

* Prometheus endpoints can serve metric data in either text or binary form. The agent automatically supports both - there is no special configuration needed. The agent will detect what form the data is in when the endpoint returns it and parses the data accordingly.
* If this is an endpoint running in an OpenShift pod (and thus this endpoint configuration is found in a configmap), you do not specify a full URL; instead you specify the protocol, port, and path and the pod's IP will be used for the hostname. URLs are only specified for those endpoints running outside of OpenShift.
* The agent supports either http or https endpoints. If the Prometheus endpoint is over the https protocol, you must configure
the agent with a certificate and private key. This is done by either starting the agent with the two environment variables `HAWKULAR_OPENSHIFT_AGENT_CERT_FILE` and `HAWKULAR_OPENSHIFT_AGENT_PRIVATE_KEY_FILE` or via the Indentity section of the agent's configuration file:
[source,yaml]
----
identity:
  cert_file: /path/to/file.crt
  private_key_file: /path/to/file.key
----
* The credentials are optional. If the Prometheus endpoint does require authorization, you can specify the credentials as either a bearer token or a basic username/password.
* A metric "id" is used when storing the metric to Hawkular Metrics. If you do not specify an "id" for a metric, its "name" will be used as the default.

== Jolokia Endpoints

A full Jolokia endpoint configuration can look like this:

[source,yaml]
----
- type: "jolokia"
  # If this is an endpoint within an OpenShift pod:
  protocol: https
  port: 8080
  path: /jolokia
  # If this is an endpoint running outside of OpenShift:
  #url: "https://yourcorp.com:8080/jolokia"
  credentials:
    token: your-bearer-token-here
    #username: your-user
    #password: your-pass
  collection_interval_secs: 300
  metrics:
  - name: java.lang:type=Threading#ThreadCount
    type: counter
    id:   VM Thread Count
  - name: java.lang:type=Memory#HeapMemoryUsage#used
    type: gauge
    id:   VM Heap Memory Used
----

Some things to note about configuring your Jolokia endpoints:

* If this is an endpoint running in an OpenShift pod (and thus this endpoint configuration is found in a configmap), you do not specify a full URL; instead you specify the protocol, port, and path and the pod's IP will be used for the hostname. URLs are only specified for those endpoints running outside of OpenShift.
* The agent supports either http or https endpoints. If the Jolokia endpoint is over the https protocol, you must configure
the agent with a certificate and private key. This is done by either starting the agent with the two environment variables `HAWKULAR_OPENSHIFT_AGENT_CERT_FILE` and `HAWKULAR_OPENSHIFT_AGENT_PRIVATE_KEY_FILE` or via the Indentity section of the agent's configuration file:
[source,yaml]
----
identity:
  cert_file: /path/to/file.crt
  private_key_file: /path/to/file.key
----
* The credentials are optional. If the Jolokia endpoint does require authorization, you can specify the credentials as either a bearer token or a basic username/password.
* A metric "id" is used when storing the metric to Hawkular Metrics. If you do not specify an "id" for a metric, its "name" will be used as the default.
* You must specify a metric's "type" as either "counter" or "gauge".
* A metric "id" is used when storing the metric to Hawkular Metrics. If you do not specify an "id" for a metric, its "name" will be used as the default.
* A metric "name" follows a strict format. First is the full MBean name (e.g. `java.lang:type=Threading`) followed by a hash (#) followed by the attribute that contains the metric data (e.g. `ThreadCount`). If the attribute is a composite attribute, then you must append a second hash followed by the composite attribute's subpath name which contains the actual metric value. For example, `java.lang:type=Memory#HeapMemoryUsage#used` will collect the `used` value of the composite attribute `HeapMemoryUsage` from the MBean `java.lang:type=Memory`.

== Environment Variables

Many of the agent's configuration settings can optionally be set via environment variables. If one of the environment variables below are set, they serve as the default value for its associated YAML configuration setting. The following are currently supported:

[cols="1,1a"]
|===
|Environment Variable Name|YAML Setting

|HAWKULAR_SERVER_URL
|
[source,yaml]
----
hawkular_server:
  url: VALUE
----

|HAWKULAR_SERVER_TENANT
|
[source,yaml]
----
hawkular_server:
  tenant: VALUE
----

|HAWKULAR_SERVER_USERNAME
|
[source,yaml]
----
hawkular_server:
  credentials:
    username: VALUE
----

|HAWKULAR_SERVER_PASSWORD
|
[source,yaml]
----
hawkular_server:
  credentials:
    password: VALUE
----

|HAWKULAR_SERVER_TOKEN
|
[source,yaml]
----
hawkular_server:
  credentials:
    token: VALUE
----

|HAWKULAR_OPENSHIFT_AGENT_CERT_FILE
|
[source,yaml]
----
identity:
  cert_file: VALUE
----

|HAWKULAR_OPENSHIFT_AGENT_PRIVATE_KEY_FILE
|
[source,yaml]
----
identity:
  private_key_file: VALUE
----

|K8S_MASTER_URL
|
[source,yaml]
----
kubernetes:
  master_url: VALUE
----

|K8S_POD_NAMESPACE
|
[source,yaml]
----
kubernetes:
  pod_namespace: VALUE
----

|K8S_POD_NAME
|
[source,yaml]
----
kubernetes:
  pod_name: VALUE
----

|K8S_TOKEN
|
[source,yaml]
----
kubernetes:
  token: VALUE
----

|K8S_CA_CERT_FILE
|
[source,yaml]
----
kubernetes:
  ca_cert_file: VALUE
----
|===

== Metric Tags

Metric data can be tagged with additional metadata called _tags_. A metric tag is a simple name/value pair. Tagging metrics allows you to further describe the metric and allows you to query for metric data based on tag queries. For more information on tags and querying tagged metric data, see the Hawkular-Metrics documentation.

Hawkular OpenShift Agent can be configured to attach custom tags to the metrics it collects. There are three places where you can define custom tags in Hawkular OpenShift Agent:

* In the global configuration of the agent (all tags defined here will be attached to all metrics stored by the agent)
* In an endpoint configuration (all tags defined here will be attached to all metrics collected from that endpoint)
* In a metric configuration (all tags defined here will only be attached to the metric)

To define global tags, you would add a top-level `tags` section in the global agent configuration file. The following configuration snippet will tell the agent to attach the tags "my-tag" (with value "my-tag-value") and "another-tag" (with value "another-tag-value") to each and every metric the agent collects.

[source,yaml]
----
tags:
- my-tag: my-tag-value
- another-tag: another-tag-value
----

To define endpoint tags (that is, tags that will be attached to every metric collected from the endpoint), you would add a `tags` section within the endpoint configuration. The following configuration snippet will tell the agent to attach the tags "my-endpoint-tag" and "my-other-endpoint-tag" to every metric that is collected from this specific Jolokia endpoint:

[source,yaml]
----
endpoints:
- type: jolokia
  tags:
    my-endpoint-tag: the-endpoint-tag-value
    my-other-endpoint-tag: the-endpoint-tag-value
----

To define tags on individual metrics, you would add a `tags` section within a metric configuration. The following configuration snippet will tell the agent to attach the tags "my-metric-tag" and "my-other-metric-tag" to the metric named "java.lang.type=Threading#ThreadCount" that is collected from this specific Jolokia endpoint:

[source,yaml]
----
endpoints:
- type: jolokia
  metrics:
  - name: java.lang.type=Threading#ThreadCount
    type: gauge
    tags:
      my-metric-tag: the-metric-tag-value
      my-other-metric-tag: the-metric-tag-value
----

Tag values can be defined with token expressions in the form of `${var}` or `$var` where _var_ is either an agent environment variable name or (if the tag definition is found in an OpenShift configmap entry) one of the following:

[cols="1,1a"]
|===
|Token Name|Description

|POD:node_name
|The name of the node where the metric was collected from.

|POD:node_uid
|The unique ID of the node where the metric was collected from.

|POD:namespace_name
|The name of the namespace of the pod where the metric was collected from.

|POD:namespace_uid
|The unique ID of the namespace of the pod where the metric was collected from.

|POD:name
|The name of the pod where the metric was collected from.

|POD:uid
|The UID of the pod where the metric was collected from.

|POD:ip
|The IP address allocated to the pod where the metric was collected from.

|POD:host_ip
|The IP address of the host to which the pod is assigned.
|===

For example:

[source,yaml]
----
tags:
  my-user-tag: my-agent-user=$USER
  my-pod-name: ${POD:name}
  some-env-tag: ${SOME_ENV_VAR}
----
